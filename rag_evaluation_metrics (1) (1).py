# -*- coding: utf-8 -*-
"""RAG Evaluation Metrics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hhHPKsOPBi02J0zWadHcj7xiZ2irnrpp
"""

import os
import re
import json
from typing import Dict, List, Tuple
from tqdm import tqdm

from rag_query import load_rag_components, retrieve_context, generate_answer

ENCODER_PATH = "./CodeRAG_encoder"
INDEX_FILE = "sql_rag_faiss_index.bin"
CORPUS_MAP_FILE = "sql_rag_corpus_map.npy"
LLM_MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.2"

EVAL_DATA_FILE = "evaluation data set.json"

EVAL_RESULTS_FILE = "rag_evaluation_results.json"


def load_evaluation_data_from_json(file_path: str) -> List[Dict[str, str]]:
    """Loads the evaluation data from the uploaded JSON file."""
    print(f"Loading evaluation data from: {file_path}")
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Evaluation file not found at {file_path}")

    with open(file_path, 'r') as f:
        data = json.load(f)

    required_keys = {'question', 'sql'}
    if not all(required_keys.issubset(set(item.keys())) for item in data):
        raise ValueError(f"Each entry in the JSON dataset must contain keys: {required_keys}")

    print(f"Loaded {len(data)} samples for evaluation.")
    return data

def save_results_to_json(results: List[Dict], filepath: str):
    """Saves the detailed per-sample evaluation results to a JSON file."""
    try:
        with open(filepath, 'w') as f:
            json.dump(results, f, indent=4)
        print(f"\n[INFO] Detailed evaluation results saved to: {filepath}")
    except Exception as e:
        print(f"\n[ERROR] Failed to save results to {filepath}: {e}")

def normalize_sql(sql_query: str) -> str:
    """
    Normalizes SQL queries for robust Execution (EX) Accuracy comparison.
    This includes:
    1. Removing SQL comments (lines starting with --).
    2. Standardizing whitespace (collapsing multiple spaces/newlines into one space).
    3. Converting to uppercase (for case-insensitive comparison of keywords/table names).
    4. Removing surrounding quotes and trailing semicolon.
    """
    sql_query = re.sub(r'--.*', '', sql_query)

    sql_query = re.sub(r'\s+', ' ', sql_query).strip()

    sql_query = sql_query.upper()

    sql_query = sql_query.rstrip(';').strip("'").strip('"').strip()
    return sql_query

def generate_results_table(all_results: List[Dict]) -> str:
    """
    Generates a Markdown table summarizing the per-sample evaluation results.
    """
    if not all_results:
        return "No results to display."

    header = ["ID", "Question", "Ground Truth SQL", "Generated SQL", "EM Match", "EX Match"]

    max_content_width = 80

    table_lines = []

    header_line = "| " + " | ".join(header) + " |"
    table_lines.append(header_line)

    separator_line = "|:" + "---:|:" + "--- |" * 2 + ":--- |:---:|"
    table_lines.append(separator_line)

    for i, result in enumerate(all_results):
        q = result['question'][:max_content_width] + ('...' if len(result['question']) > max_content_width else '')
        gt = result['ground_truth'][:max_content_width] + ('...' if len(result['ground_truth']) > max_content_width else '')
        gen = result['generated_sql'][:max_content_width] + ('...' if len(result['generated_sql']) > max_content_width else '')

        row_line = f"| {i+1} | {q} | {gt} | {gen} | {em_display} | {ex_display} |"
        table_lines.append(row_line)

    return "\n".join(table_lines)


def evaluate_rag_pipeline(eval_data: List[Dict[str, str]]) -> Tuple[Dict[str, float], List[Dict]]:
    """
    Runs the RAG pipeline over the evaluation dataset and calculates EM and EX Accuracy,
    and collects detailed per-sample results.
    """

    try:
        encoder, index, corpus, llm_model, llm_tokenizer = load_rag_components(
            ENCODER_PATH, INDEX_FILE, CORPUS_MAP_FILE, LLM_MODEL_NAME
        )
    except Exception as e:
        print(f"\n[CRITICAL ERROR] Failed to load RAG components: {e}")
        return {"EM_Accuracy": 0.0, "EX_Accuracy": 0.0}, []

    total_samples = len(eval_data)
    exact_match_hits = 0
    execution_match_hits = 0
    all_results = []

    print("\nStarting RAG Evaluation...")
    for sample in tqdm(eval_data, desc="Evaluating Samples"):
        query = sample['question']
        ground_truth_sql = sample['sql']

        retrieved_context = retrieve_context(query, encoder, index, corpus, top_k=3)
        generated_sql = generate_answer(query, retrieved_context, llm_model, llm_tokenizer)

        normalized_ground_truth = normalize_sql(ground_truth_sql)
        normalized_generated_sql = normalize_sql(generated_sql)

        em_correct = generated_sql.strip() == ground_truth_sql.strip()
        if em_correct:
            exact_match_hits += 1

        ex_correct = normalized_generated_sql == normalized_ground_truth
        if ex_correct:
            execution_match_hits += 1

        all_results.append({
            'question': query,
            'ground_truth': ground_truth_sql,
            'generated_sql': generated_sql,
            'retrieved_context': retrieved_context,
            'em_correct': em_correct,
            'em_loss': 0.0 if em_correct else 1.0,
            'ex_correct': ex_correct,
            'ex_loss': 0.0 if ex_correct else 1.0
        })

    em_accuracy = exact_match_hits / total_samples
    ex_accuracy = execution_match_hits / total_samples

    summary_metrics = {
        "EM_Accuracy": em_accuracy,
        "EX_Accuracy": ex_accuracy
    }

    return summary_metrics, all_results


def main():
    if not os.path.exists(ENCODER_PATH):
        print(f"[ERROR] Encoder path '{ENCODER_PATH}' not found. Please run your fine-tuning script first.")
        return
    if not os.path.exists(INDEX_FILE):
        print(f"[ERROR] FAISS index file '{INDEX_FILE}' not found. Please run 'vectorbase.py' first.")
        return
    if not os.path.exists(EVAL_DATA_FILE):
        print(f"[ERROR] Evaluation data file '{EVAL_DATA_FILE}' not found.")
        return

    try:
        eval_data = load_evaluation_data_from_json(EVAL_DATA_FILE)

        summary_metrics, all_results = evaluate_rag_pipeline(eval_data)

        save_results_to_json(all_results, EVAL_RESULTS_FILE)

        print("\n" + "="*50)
        print("RAG EVALUATION SUMMARY RESULTS")
        print(f"Total Samples: {len(eval_data)}")
        print(f"1. Exact Match (EM) Accuracy: {summary_metrics['EM_Accuracy'] * 100:.2f}%")
        print(f"   Exact Match (EM) Loss: { (1 - summary_metrics['EM_Accuracy']) * 100:.2f}%")
        print(f"2. Execution (EX) Accuracy (Normalized SQL Proxy): {summary_metrics['EX_Accuracy'] * 100:.2f}%")
        print(f"   Execution (EX) Loss (Normalized SQL Proxy): { (1 - summary_metrics['EX_Accuracy']) * 100:.2f}%")
        print("="*50)

        print("\n\n" + "="*50)
        print("RAG EVALUATION DETAILED RESULTS (Per Sample)")
        print("="*50)
        detailed_table = generate_results_table(all_results)
        print(detailed_table)
        print("="*50)

    except Exception as e:
        print(f"\n[ERROR] Evaluation failed due to an unexpected error: {e}")

if __name__ == "__main__":
    main()